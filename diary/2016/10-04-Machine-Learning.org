* 基于树的回归

当数据有很多特性, 而且这些特性之间有着复杂的内部相互作用的时候, 建立一个全局的模型不是非常的不明智就是很困难. 特性直接有相互作用, 用函数的语言来说, 就是函数不是线性函数, 变量直接有相互的依赖; 用矩阵的语言来说, 就是特性向量之间不是相互的独立的. 有些可以通过组合来得到其他的. 这就是说有这些特性组成的矩阵将不是满秩的. 也就是说线性模型的假设是不成立的. 大部分的实际情况, 都不是线性的. 所以, 使用线性模式对于大部分实际的情况其实是不合适的. 尤其是对于全局, 对整体都使用线性模型来说.

所以为数据建立模型的一个可行的方法, 就是把数据分割成可以用简单模型处理的片段. 对于一个数据片段可以用线性模型来处理, 那么这个片段就用线性模型处理, 如果片段还是不能用线性模型来处理, 那就继续的把这个片段再继续的划分成更小的片段, 直到片段可以用线性模型来处理为止.

构建树的第一个算法叫做 CART(Classsification And Regression Tress, 分类与回归树), 它可以用来做回归或分类. 是一个非常有用的工具.

** 复杂数据的局部建模

在构建决策树的代码的时候, 使用的是 ID3 算法. ID3 算法选择一个最好的特性来分割数据，按照这个特性的所有取值来分割数据成为几个不同的片段. 比如, 如果一个特性有 4 个可取的值, 那么数据就被分成 4 份. 数据被分割之后, 这个特性就被从分割候选项中删除了. 另外的分割数据的方法是把数据做一个二分. 如果数据的特性对于一个特定的值, 那么就把它放在树的左边; 否则就放在右边.

ID3 算法还有一个限制: 它不能处理连续的特性. 要想ID3 算法处理连续的数据, 必须首先把连续特征离散化. 这种量子化丢掉了连续变量的一些固有的特征. 二分分割允许我们很容易的来调整我们的构建树的算法, 以处理连续的特征. 要处理一个连续的变量, 我们选择的特征; 值如果大于我们期望的, 就把这个值放在数的左边, 其他的值放在树的右边.

** 使用连续和离散的特征来构建树

要构建一棵树, 我们需要一个方法, 可以存储构建一个树的不同类型的数据. 我们使用一个字典来作为我们树数据的结构. 这个字典有一下四个字段:
+ 特性 :: 一个表示用来分割数据的特征的符号.~spInd~
+ 值 :: 上面特征的值, 用这个值来判断数据怎么分割.~spVal~
+ 右边 :: 右子树; 这也可能只是一个值 如果算法决定我们不需要进一步的分割数据的话.~left~
+ 左边 :: 左子树和右子树类似.~right~

createTree() 的伪代码为:
#+BEGIN_SRC text
找到分割的最好的特性
  如果  不能分割数据, 这个节点编程一个叶子节点
  把数据做一个二分分割
  用分割出来的右边的数据调用 createTree()
  用分割出来的左边的数据调用 createTree()
#+END_SRC
